---
title: "Simple Normal Regression"
author: "Dr. Mine Dogucu"
execute:
  echo: true
format: 
  revealjs:
    footer: "[stats115.com](https://stats115.com)"
    slide-number: true
    incremental: true
    theme: ["../templates/slide-style.scss"]
    logo: "https://www.stats115.com/img/logo.png"
    title-slide-attributes: 
      data-background-image: "https://stats115.com/img/logo.png"
      data-background-size: 5%
      data-background-position: 50% 85%
    include-after-body: "../templates/clean_title_page.html"
---

```{r}
#| echo: false
library(bayesrules)
library(tidyverse)
library(rstanarm)
library(bayesplot)
library(janitor)
library(rstan)
library(gridExtra)
theme_set(theme_gray(base_size = 18))
```

##

The notes for this lecture are derived from  [Chapter 9 of the Bayes Rules! book](https://www.bayesrulesbook.com/chapter-9)


##

```{r}
glimpse(bikes)
```


## Rides

:::{.pull-left}


```{r echo = FALSE, fig.height=5}
ggplot(data.frame(x = c(-4,4)), aes(x=x)) + 
  stat_function(fun = dnorm) + 
  labs(y = expression(paste("f(y|",mu,",",sigma,")")), x = "y (rides)") + 
  scale_x_continuous(breaks = c(-3,0,3), labels = c(expression(paste(mu,"- 3 / ",sigma)), expression(mu), expression(paste(mu,"+ 3 / ",sigma))))
```

:::

:::{.pull-right}[

$Y_i | \mu, \sigma  \stackrel{ind}{\sim} N(\mu, \sigma^2)$  
$\mu \sim N(\theta, \tau^2)$
$\sigma  \sim \text{ some prior model.}$

:::



## Regression Model

$Y_i$ the number of rides  
$X_i$ temperature (in Fahrenheit) on day $i$. 

. . .

$\mu_i = \beta_0 + \beta_1X_i$

. . .

$\beta_0:$ the typical ridership on days in which the temperature was 0 degrees ( $X_i$=0). It is not interpretable in this case.

$\beta_1:$ the typical change in ridership for every one unit increase in temperature.



## Normal likelihood model

\begin{split}
Y_i | \beta_0, \beta_1, \sigma & \stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right) \;\; \text{ with } \;\; \mu_i = \beta_0 + \beta_1X_i \; .\\
\end{split}

```{r}
#| echo: false
#| fig-align: center
set.seed(454)
x <- rnorm(100, mean = 68, sd = 12)
y_1 <- -2511 + 88*x + rnorm(100, mean=0, sd = 2000)
y_2 <- -2511 + 88*x + rnorm(100, mean=0, sd = 200)
bikes_sim <- data.frame(x, y_1, y_2) %>% filter(y_1 > 0)
g1 <- ggplot(bikes_sim, aes(x=x,y=y_1)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) + 
    #scale_x_continuous(breaks = c(25)) + 
    #scale_y_continuous(breaks = c(0,30), limits = c(min(y_1,y_2),max(y_1,y_2))) +
    lims(y = c(min(y_1,y_2),max(y_1,y_2))) + 
    labs(x = "x (temp)", y = "y (rides)")
g2 <- ggplot(bikes_sim, aes(x=x,y=y_2)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) + 
    #scale_x_continuous(breaks = c(25)) + 
    #scale_y_continuous(breaks = c(30), limits = c(min(y_1,y_2),max(y_1,y_2))) + 
    lims(y = c(min(y_1,y_2),max(y_1,y_2))) + 
    labs(x = "x (temp)", y = "y (rides)")
    
gridExtra::grid.arrange(g1,g2,ncol=2)
  
```

##

These simulations show two cases where $\beta_0 = -2000$ and slope $\beta_1 = 100$.
On the left $\sigma = 2000$ and on the right $\sigma = 200$ (right). In both cases, the model line is defined by $\beta_0 + \beta_1 x = -2000 + 100 x$.



## Prior Models

$\text{likelihood model:} \; \; \; Y_i | \beta_0, \beta_1, \sigma \;\;\;\stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right)\text{ with } \mu_i = \beta_0 + \beta_1X_i$

$\text{prior models:}$ 

$\beta_0\sim N(m_0, s_0^2 )$  
$\beta_1\sim N(m_1, s_1^2 )$  
$\sigma \sim \text{Exp}(l)$


Recall: 

$\text{Exp}(l) = \text{Gamma}(1, l)$


##


```{r fig.height=5}
plot_normal(mean = 5000, sd = 1000) + 
  labs(x = "beta_0c", y = "pdf")
```

##




```{r fig.height=5}
plot_normal(mean = 100, sd = 40) + 
  labs(x = "beta_1", y = "pdf")
```

##




```{r fig.height=5}
plot_gamma(shape = 1, rate = 0.0008) + 
  labs(x = "sigma", y = "pdf")
```

##




$$\begin{split}
Y_i | \beta_0, \beta_1, \sigma & \stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right) \;\; \text{ with } \;\; \mu_i = \beta_0 + \beta_1X_i \\
\beta_{0c}  & \sim N\left(5000, 1000^2 \right)  \\
\beta_1  & \sim N\left(100, 40^2 \right) \\
\sigma   & \sim \text{Exp}(0.0008)  .\\
\end{split}$$

##

 

```{r echo = FALSE, warning=FALSE, fig.height=6, fig.width=15}
g1 <- plot_normal(mean = 5000, sd = 1000) + 
  labs(x = "beta_0c", y = "pdf")
g2 <- plot_normal(mean = 100, sd = 40) + 
  labs(x = "beta_1", y = "pdf")
g3 <- plot_gamma(shape = 1, rate = 0.0008) + 
  labs(x = "sigma", y = "pdf") + 
  lims(x = c(0,7500))
gridExtra::grid.arrange(g1,g2,g3,ncol=3)
```



## Simulation via `rstanarm`

```{r cache=TRUE}
bike_model <- stan_glm(rides ~ temp_feel, data = bikes,
                       family = gaussian,
                       prior_intercept = normal(5000, 1000),
                       prior = normal(100, 40), 
                       prior_aux = exponential(0.0008),
                       chains = 4, iter = 5000*2, seed = 84735,
                       refresh = FALSE) 
```

The `refresh = FALSE` prevents printing out your chains and iterations, especially useful in R Markdown.

##



```{r}
# Effective sample size ratio and Rhat
neff_ratio(bike_model)

rhat(bike_model)

```

The effective sample size ratios are slightly above 1 and the R-hat values are very close to 1, indicating that the chains are stable, mixing quickly, and behaving much like an independent sample.

##



```{r fig.width=12}
mcmc_trace(bike_model, size = 0.1)
```

##




```{r fig.width=12, fig.height=6}
mcmc_dens_overlay(bike_model)
```

##

```{r}
# STEP 1: DEFINE the model
stan_bike_model <- "
  data {
    int<lower = 0> n;
    vector[n] Y;
    vector[n] X;
  }
  parameters {
    real beta0;
    real beta1;
    real<lower = 0> sigma;
  }
  model {
    Y ~ normal(beta0 + beta1 * X, sigma);
    beta0 ~ normal(-2000, 1000);
    beta1 ~ normal(100, 40);
    sigma ~ exponential(0.0008);
  }
"
```

##



```{r cache=TRUE}
# STEP 2: SIMULATE the posterior
stan_bike_sim <- 
  stan(model_code = stan_bike_model, 
       data = list(n = nrow(bikes), Y = bikes$rides, X = bikes$temp_feel), 
       chains = 4, iter = 5000*2, seed = 84735, refresh = FALSE)
```

##

```{r}
broom.mixed::tidy(bike_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)
```


```{r echo = FALSE}
model_summary <- broom.mixed::tidy(bike_model, effects = c("fixed", "aux"),
                      conf.int = TRUE, conf.level = 0.80)
b0_median <- model_summary[1,2]
b1_median <- model_summary[2,2]
b1_lower <- model_summary[2,4]
b1_upper <- model_summary[2,5]
```

Referring to the `tidy()` summary, the __posterior median relationship__ is

$$\begin{equation}
`r round(b0_median,2)` + `r round(b1_median,2)` X
\end{equation}$$

##



```{r}
# Store the 4 chains for each parameter in 1 data frame
bike_model_df <- as.data.frame(bike_model)
# Check it out
nrow(bike_model_df)
head(bike_model_df, 3)
```

##



```{r warning=FALSE, fig.height=4}
# 50 simulated model lines
bikes %>%
  tidybayes::add_fitted_draws(bike_model, n = 50) %>%
  ggplot(aes(x = temp_feel, y = rides)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + 
    geom_point(data = bikes, size = 0.05)
```


##


 

```{r}
# Tabulate the beta_1 values that exceed 0
bike_model_df %>% 
  mutate(exceeds_0 = temp_feel > 0) %>% 
  tabyl(exceeds_0)
```



## Posterior Prediction

Suppose a weather report indicates that tomorrow will be a 75-degree day in D.C. What's your posterior guess of the number of riders that Capital Bikeshare should anticipate?


##



```{r echo = FALSE}
pred <- round(b0_median,2) + (round(b1_median,2)*75)
```


Your natural first crack at this question might be to plug the 75-degree temperature into the posterior median model.
Thus, we expect that there will be `r round(pred)` riders tomorrow:

$$`r round(b0_median,2)` + `r round(b1_median,2)`*75 = `r pred`$$

. . .

Not quiet.

##



Recall that this singular prediction ignores two potential sources of variability:

- __Sampling variability__ in the data    
    The observed ridership outcomes, $Y$, typically _deviate_ from the model line. That is, we don't expect every 75-degree day to have the same exact number of rides.
    
. . .
    
- __Posterior variability__ in parameters $(\beta_0, \beta_1, \sigma)$    

## 

The posterior median model is merely the center in a _range_ of plausible model lines $\beta_0 + \beta_1 X$. We should consider this entire range as well as that in $\sigma$, the degree to which observations might deviate from the model lines.

. . .

The __posterior predictive model__ of a new data point $Y_{\text{new}}$ accounts for both sources of variability.


##

We have20,000 sets of parameters in the Markov chain $\left(\beta_0^{(i)},\beta_1^{(i)},\sigma^{(i)}\right)$.
We can then _approximate_ the posterior predictive model for $Y_{\text{new}}$ at $X = 75$ by simulating a ridership prediction from the Normal model evaluated each parameter set:

$$Y_{\text{new}}^{(i)} | \beta_0, \beta_1, \sigma  \; \sim \; N\left(\mu^{(i)}, \left(\sigma^{(i)}\right)^2\right) \;\; \text{ with } \;\; \mu^{(i)} = \beta_0^{(i)} + \beta_1^{(i)} \cdot 75.$$

##

$$\left[
\begin{array}{lll} 
\beta_0^{(1)} & \beta_1^{(1)} & \sigma^{(1)} \\
\beta_0^{(2)} & \beta_1^{(2)} & \sigma^{(2)} \\
\vdots & \vdots & \vdots \\
\beta_0^{(20000)} & \beta_1^{(20000)} & \sigma^{(20000)} \\
\end{array}
\right]
\;\; \longrightarrow \;\;
\left[
\begin{array}{l} 
Y_{\text{new}}^{(1)} \\
Y_{\text{new}}^{(2)} \\
\vdots \\
Y_{\text{new}}^{(20000)} \\
\end{array}
\right]$$


##

 

```{r}
first_set <- head(bike_model_df, 1)
first_set
```


##

 

```{r}
mu <- first_set$`(Intercept)` + first_set$temp_feel * 75
mu
```


##

 


```{r}
set.seed(84735)
y_new <- rnorm(1, mean = mu, sd = first_set$sigma)
y_new
```

##

 

```{r}
# Predict rides for each parameter set in the chain
set.seed(84735)
predict_75 <- bike_model_df %>% 
  mutate(mu = `(Intercept)` + temp_feel*75,
         y_new = rnorm(20000, mean = mu, sd = sigma))
```


```{r}
head(predict_75, 3)

```

##

 

```{r}
# Construct 95% posterior credible intervals
predict_75 %>% 
  summarize(lower_mu = quantile(mu, 0.025),
            upper_mu = quantile(mu, 0.975),
            lower_new = quantile(y_new, 0.025),
            upper_new = quantile(y_new, 0.975))
```


##

 

```{r eval = FALSE}
# Plot the posterior model of the typical ridership on 75 degree days
ggplot(predict_75, aes(x = mu)) + 
  geom_density()
# Plot the posterior predictive model of tomorrow's ridership
ggplot(predict_75, aes(x = y_new)) + 
  geom_density()
```


##

 

```{r ch9-post-pred, echo = FALSE}
g1 <- ggplot(predict_75, aes(x = mu)) + 
  geom_density() +
  lims(x = range(predict_75$y_new), y = c(0, 0.0065))
g2 <- ggplot(predict_75, aes(x = y_new)) + 
  geom_density() +
  lims(x = range(predict_75$y_new), y = c(0, 0.0065))
gridExtra::grid.arrange(g1,g2,ncol=2)
```

##

 

## Posterior Prediction with rstanarm

```{r}
# Simulate a set of predictions
set.seed(84735)
shortcut_prediction <- 
  posterior_predict(bike_model, newdata = data.frame(temp_feel = 75))
```

```{r}
head(shortcut_prediction, 3)
```

##

 


```{r}
# Construct a 95% posterior credible interval
posterior_interval(shortcut_prediction, prob = 0.95)
```

##

 

```{r fig.height=5}
# Plot the approximate predictive model
mcmc_dens(shortcut_prediction) + 
  xlab("predicted ridership on a 75 degree day")
```

##



## Using the default priors in `rstanarm`

```{r}
bike_model_default <- stan_glm(
  rides ~ temp_feel, data = bikes, 
  family = gaussian,
  prior_intercept = normal(5000, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, seed = 84735, refresh=FALSE)
```

By setting `autoscale = TRUE`, `stan_glm()` adjusts or scales our default priors to optimize the study of parameters which have different scales.

##



```{r}
prior_summary(bike_model_default)
```


##



## Default vs non-default priors

Con: weakly informative priors are tuned with information from the data (through a fairly minor consideration of scale). 

Pro: Unless we have strong prior information, utilizing the defaults will typically lead to more stable simulation results than if we tried tuning our own vague priors.

Pro: The defaults can help us get up and running with Bayesian modeling. In future lectures, we’ll often utilize the defaults in order to focus on the new modeling concepts.



