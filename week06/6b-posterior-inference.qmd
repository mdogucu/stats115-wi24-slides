---
title: "Posterior Inference and Prediction"
author: "Dr. Mine Dogucu"
execute:
  echo: true
format: 
  revealjs:
    footer: "[stats115.com](https://stats115.com)"
    slide-number: true
    incremental: true
    theme: ["../templates/slide-style.scss"]
    logo: "https://www.stats115.com/img/logo.png"
    title-slide-attributes: 
      data-background-image: "https://stats115.com/img/logo.png"
      data-background-size: 5%
      data-background-position: 50% 85%
    include-after-body: "../templates/clean_title_page.html"
---

```{r}
#| echo: false
library(bayesrules)
library(tidyverse)
library(janitor)
library(ggforce)
library(gridExtra)
theme_set(theme_gray(base_size = 18))
```

##

The notes for this lecture are derived from  [Chapter 8 of the Bayes Rules! book](https://www.bayesrulesbook.com/chapter-8)

##



```{r}
library(bayesrules)
data(moma_sample)
glimpse(moma_sample)
```

##



$$Y|\pi  \sim \text{Bin}(100, \pi)$$ 

$$\pi  \sim \text{Beta}(4, 6)$$

##

```{r}
moma_sample |> 
  group_by(genx) |> 
  tally()
```

##



$$\begin{split}
Y | \pi & \sim \text{Bin}(100, \pi) \\
\pi & \sim \text{Beta}(4, 6) \\
\end{split} \;\;\;\; \Rightarrow \;\;\;\; \pi | (Y = 14) \sim \text{Beta}(18, 92)$$

##

```{r fig.height=5, fig.width=9}
plot_beta_binomial(alpha = 4, beta = 6, y = 14, n = 100)
```

##

```{r echo=FALSE, fig.width=12}
# Keep: these references are woven throughout the discussion

# Data
y <- 14
n <- 100

# Prior parameters
prior_a <- 4
prior_b <- 6

# Posterior parameters
post_a <- prior_a + y
post_b <- prior_b + n - y

# Posterior trend
post_mean_unrounded <- round(post_a / (post_a + post_b),3)
post_mean <- round(post_mean_unrounded, 2)
post_mode <- round((post_a - 1) / (post_a + post_b - 2), 2)

# 95% CI
qs <- qbeta(c(0.025, 0.975), post_a, post_b)
lower_95 <- round(qs[1],2)
upper_95 <- round(qs[2],2)

# 99% CI
qs <- qbeta(c(0.005, 0.995), post_a, post_b)
lower_99 <- round(qs[1],2)
upper_99 <- round(qs[2],2)

# Probabilities
post_prob <- pbeta(0.20, post_a, post_b)
prior_prob <- pbeta(0.20, prior_a, prior_b)
new_y <- 0
new_n <- 10
new_post_a <- prior_a + new_y
new_post_b <- prior_b + new_n - new_y
qs <- qbeta(c(0.025, 0.975), new_post_a, new_post_b)
new_lower_95 <- round(qs[1],2)
new_upper_95 <- round(qs[2],2)

plot_fun <- function(a,b,x,n,level){
  q1  <- (1 - level)/2
  q2  <- 1 - q1
  p_a <- a + x
  p_b <- b + n - x
  ci  <- qbeta(c(q1,q2), p_a, p_b)
  post_mode <- (p_a - 1) / (p_a + p_b - 2)
  marks <- c(ci, post_mode)
  ggplot(data.frame(x = c(0.4,1)), aes(x=x)) + 
    stat_function(fun = dbeta, args = list(p_a, p_b), xlim = ci, geom = "area", fill = "lightblue") + 
    stat_function(fun = dbeta, args = list(p_a, p_b)) + 
    geom_segment(data = data.frame(x = marks, y1 = c(0,0,0), y2 = dbeta(marks, p_a, p_b)),
                 aes(x = x, xend = x, y = y1, yend = y2)) +
    labs(x = expression(pi), y = "density") + 
    lims(y = c(0,12), x = c(0,0.6))
}

plot_fun(4, 4, y, n, level = 0.95) +
  labs(y = expression(paste("f(",pi,"|Y=14)")))
```

##



```{r}
summarize_beta_binomial(4, 6, y = 14, n = 100)
```




## 95% Posterior Credible Interval (CI)

0.025th & 0.975th quantiles of the Beta(18,92) posterior

```{r}
qbeta(c(0.025, 0.975), 18, 92)
```

. . .

$\int_{0.1}^{0.24} f(\pi|(y=14)) d\pi  \; \approx \; 0.95$

##

```{r post-ci-ch8, fig.width = 10, echo = FALSE, message=FALSE}
# Alt text: The same roughly symmetric density curve, which ranges from roughly 0.05 to 0.3 is plotted three times. In the left curve, a narrow area between 0.14 and 0.19 is shaded in. In the middle curve, a wider area between 0.1 and 0.24 is shaded in. In the right curve, the widest area between 0.08 and 0.26 is shaded in.
g1 <- plot_fun(prior_a, prior_b, y, n, level = 0.50) + lims(x = c(0,0.32)) +
  labs(title = "50% posterior CI")
g2 <- plot_fun(prior_a, prior_b, y, n, level = 0.95) + lims(x = c(0,0.32)) +
  labs(title = "95% posterior CI")
g3 <- plot_fun(prior_a, prior_b, y, n, level = 0.99) + lims(x = c(0,0.32)) +
  labs(title = "99% posterior CI")
grid.arrange(g1,g2,g3,ncol=3)
```

##

 

$$\begin{split}
H_0: & \; \; \pi \ge 0.20 \\
H_a: & \; \; \pi < 0.20 \\
\end{split}$$

##
 


```{r post-prob-ch8, echo = FALSE}
plot_prob_plot <- function(a,b,cutoff){
  ggplot(data.frame(x = c(0,1)), aes(x=x)) + 
    stat_function(fun = dbeta, args = list(a, b), xlim = c(0,cutoff), geom = "area", fill = "lightblue") + 
    stat_function(fun = dbeta, args = list(a, b)) + 
    geom_segment(aes(x = cutoff, xend = cutoff, y = 0, yend = dbeta(cutoff, a, b))) +
    labs(x = expression(pi), y = "density")
}

plot_prob_plot(post_a, post_b, cutoff = 0.20) + 
  lims(y = c(0,12.5), x = c(0, 0.32))
```

##

 


```{r}
# Posterior probability that pi < 0.20
post_prob <- pbeta(0.20, 18, 92)
post_prob
```

##

 

$$\text{Posterior odds } = \frac{P(H_a \; | \; (Y=14))}{P(H_0 \; | \; (Y=14))} \approx 5.62 $$  

. . .

```{r}
# Posterior odds
post_odds <- post_prob / (1 - post_prob)
post_odds
```

##

 


```{r prior-post-ch8, echo = FALSE, fig.width = 10}
g1 <-plot_prob_plot(prior_a, prior_b, cutoff = 0.20) + 
  lims(y = c(0,12.5)) + 
  labs(title = "Prior")

g2 <- plot_prob_plot(post_a, post_b, cutoff = 0.20) + 
  lims(y = c(0,12.5)) + 
  labs(title = "Posterior")

grid.arrange(g1, g2, ncol = 2)

```

##

 

$$P(\pi<0.20)$$
```{r}
prior_prob <- pbeta(0.20, 4, 6)
prior_prob
```

. . .

$$\text{Prior odds } = \frac{P(H_a)}{P(H_0)} \approx 0.093 \; .$$ 

```{r}
prior_odds <- prior_prob / (1 - prior_prob)
prior_odds
```

##

 


The __Bayes Factor (BF)__ compares the posterior odds to the prior odds, hence provides insight into just how much our understanding about Gen X representation _evolved_ upon observing our sample data:


$$\text{Bayes Factor} = \frac{\text{Posterior odds }}{\text{Prior odds }}$$

```{r echo = FALSE}
BF <- post_odds / prior_odds
```

##

 


## Bayes Factor

In a hypothesis test of two competing hypotheses, $H_a$ vs $H_0$, the Bayes Factor is an odds ratio for $H_a$:

$$\text{Bayes Factor}
= \frac{\text{Posterior odds}}{\text{Prior odds}}
= \frac{P(H_a | Y) / P(H_0 | Y)}{P(H_a) / P(H_0)}
 \; .$$

As a ratio, it's meaningful to compare the Bayes Factor (BF)\ to 1.  To this end, consider three possible scenarios:

##

1. BF = 1:  The plausibility of $H_a$ _didn't change_ in light of the observed data.
2. BF > 1:  The plausibility of $H_a$ _increased_ in light of the observed data.  Thus the greater the Bayes Factor, the more convincing the evidence for $H_a$.
3. BF < 1:  The plausibility of $H_a$ _decreased_ in light of the observed data. 

##

 


## Two-sided Tests

$$\begin{split}
H_0: & \; \; \pi = 0.3 \\
H_a: & \; \; \pi \ne 0.3 \\
\end{split}$$

$$\text{Posterior odds } = \frac{P(H_a \; | \; (Y=14))}{P(H_0 \; | \; (Y=14))} = \frac{1}{0} = \text{ nooooo!}$$

. . .

Recall 95% posterior CI

```{r}
qbeta(c(0.025, 0.975), 18, 92)
```

##

 



```{r}
#| cache: true
#| message: false
library(rstan)
# STEP 1: DEFINE the model
art_model <- "
  data {
    int<lower=0, upper=100> Y;
  }
  parameters {
    real<lower=0, upper=1> pi;
  }
  model {
    Y ~ binomial(100, pi);
    pi ~ beta(4, 6);
  }
"
# STEP 2: SIMULATE the posterior
art_sim <- stan(model_code = art_model, data = list(Y = 14), 
  chains = 4, iter = 5000*2, seed = 84735)
```

##

 

```{r message=FALSE, fig.height=5}
library(bayesplot)
# Parallel trace plots & density plots
mcmc_trace(art_sim, pars = "pi", size = 0.5)
```

##

 


```{r message=FALSE}
# Combined density plot
mcmc_dens_overlay(art_sim, pars = "pi")
```

##

 

```{r message=FALSE}
# Combined density plot
mcmc_dens(art_sim, pars = "pi")
```

##

 

```{r eval=FALSE}
summary(art_sim, pars = "pi")
```

##

 


```{r}
mcmc_areas(art_sim, pars = "pi",
  prob = 0.95, point_est = "mean")
```

##

 


```{r}
mcmc_areas(art_sim, pars = "pi",
  prob = 0.95, point_est = "mean")
```

##

 


```{r}
art_chains_df <- as.data.frame(art_sim, 
  pars ="lp__", include = FALSE)

art_chains_df |> 
  summarize(post_mean = mean(pi), 
    post_mode = sample_mode(pi),
    lower_95 = quantile(pi, 0.025),
    upper_95 = quantile(pi, 0.975))
```

##

 


```{r}
art_chains_df |> 
  mutate(exceeds = pi < 0.20) |> 
  tabyl(exceeds)
```

. . .

```{r}
post_prob
```

##



__a Bayesian analysis assesses the uncertainty regarding an unknown parameter $\pi$ in light of observed data $Y$__.


$$P((\pi < 0.20) \; | \; (Y = 14)) = `r post_prob` \;.$$

. . .

__a frequentist analysis assesses the uncertainty of the observed data $Y$ in light of assumed values of $\pi$.__

$$P((Y \le 14) | (\pi = 0.20)) = 0.08$$




## Posterior prediction of new outcomes

Suppose we get our hands on 20 more artworks.
What number would you _predict_ are done by artists that are Gen X or younger?


- __sampling variability__ 

- __posterior variability__ in $\pi$.

##



First, let $Y' = y'$ be the (yet unknown) number of the 20 new artworks that are done by Gen X or younger artists.
Thus conditioned on $\pi$, the randomness in $Y'$ can be modeled by $Y'|\pi \sim \text{Bin}(20,\pi)$ with pdf

$$f(y'|\pi) = P(Y' = y' | \pi) = \binom{20}{ y'} \pi^{y'}(1-\pi)^{20 - y'}$$

. . .



$$f(y'|\pi) f(\pi|(y=14)) \; ,$$

##




```{r art-pred-1-ch8, fig.width = 15, echo = FALSE}
binom_plot <- function(pi_val){
  pred_plot <- data.frame(x = 0:20, y = dbinom(0:20, size = 20, prob = pi_val)*dbeta(pi_val,post_a, post_b))
  ggplot(pred_plot, aes(x = x, y = y)) + 
    geom_point(size = 0.7) + 
    geom_segment(aes(x = x, xend = x, y = 0, yend = y), size = 0.1) + 
    lims(y = c(0,3)) + 
    labs(x = "y'", title = substitute(paste(pi, " = ", nn), list(nn = pi_val)), y = expression(paste("f(y'|", pi, ")", "f(", pi, "|(y = 14))")))
}


g1 <- binom_plot(pi_val = lower_95) 
g2 <- binom_plot(pi_val = post_mode)
g3 <- binom_plot(pi_val = upper_95)

grid.arrange(g1, g2, g3, ncol=3)

```

##


$f(y'|(y=14)) = \int_0^1 f(y'|\pi) f(\pi|(y=14)) d\pi$

. . .

$f(y'|(y=14)) = {20 \choose y'} \frac{\Gamma(110)}{\Gamma(18)\Gamma(92)}\frac{\Gamma(18+y')\Gamma(112-y')} {\Gamma(130)}  \text{ for } y' \in \{0,1,\ldots,20\}$

. . .

$f((y'=3)|(y=14)) = {20 \choose 3}\frac{\Gamma(110)}{\Gamma(18)\Gamma(92)}\frac{\Gamma(18+3)\Gamma(112-3)} {\Gamma(130)} = 0.2217$

##




```{r art-pred-2-ch8, echo = FALSE}
pred_pdf <- Vectorize(function(x){choose(20,x)*gamma(110)/gamma(18)/gamma(92)*gamma(18+x)*gamma(112-x) /gamma(130)})

# Confirm it's valid
#sum(pred_pdf(x = 0:20))

plot_pred <- data.frame(x = 0:20, y = pred_pdf(0:20))
ggplot(plot_pred, aes(x = x, y = y)) + 
  geom_point(size = 0.7) + 
  geom_segment(aes(x = x, xend = x, y = 0, yend = y), size = 0.25) + 
  labs(x = "y'", y = "f(y' | (y = 14))")


```

##




Let $Y'$ denote a new outcome of variable $Y$.  Further, let pdf $f(y'|\pi)$ denote the dependence of $Y'$ on $\pi$ and posterior pdf $f(\pi|y)$ denote the posterior plausibility of $\pi$ given the original data $Y = y$.  Then the posterior predictive model for $Y'$ has pdf

$$f(y'|y) = \int f(y'|\pi) f(\pi|y) d\pi \; .$$

In words, the overall chance of observing $Y' = y'$ weights the chance of observing this outcome under _any_ possible $\pi$ ( $f(y'|\pi)$ ) by the posterior plausibility of $\pi$ ( $f(\pi|y)$ ).

##



```{r echo=FALSE, cache=TRUE}
library(rstan)
# STEP 1: DEFINE the model
art_model <- "
  data {
    int<lower=0, upper=100> Y;
  }
  parameters {
    real<lower=0, upper=1> pi;
  }
  model {
    Y ~ binomial(100, pi);
    pi ~ beta(4, 6);
  }
"

# STEP 2: SIMULATE the posterior
art_sim <- stan(
  model_code = art_model, data = list(Y = 14), 
  chains = 4, iter = 5000*2, seed = 84735, refresh=FALSE)

art_chains_df <- as.data.frame(art_sim, 
  pars ="lp__", include = FALSE)
```

```{r}
head(art_chains_df, 3)
```

##




```{r}
set.seed(84735)
# Predict a value of Y' for each pi value in the chain
art_chains_df <- art_chains_df |> 
  mutate(y_predict = rbinom(length(pi), size = 20, prob = pi))
```

. . .


```{r}
head(art_chains_df, 3)
```

##



```{r fig.height=5}
ggplot(art_chains_df, aes(x = y_predict)) + 
  stat_count()
```

